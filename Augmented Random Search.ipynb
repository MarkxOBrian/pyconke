{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Hyper Parameters\n",
    "\n",
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 1000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the states (For performance purposes)\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the AI\n",
    "\n",
    "def train(env, policy, normalizer, hp):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        \n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "        \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step:', step, 'Reward:', reward_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WalkerBase::__init__ start\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HalfCheetahBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "Step: 0 Reward: -960.8180044257189\n",
      "Step: 1 Reward: -962.0348783831652\n",
      "Step: 2 Reward: -937.6406633444094\n",
      "Step: 3 Reward: -949.1764930916991\n",
      "Step: 4 Reward: -955.7390501904398\n",
      "Step: 5 Reward: -955.808016672099\n",
      "Step: 6 Reward: -926.2073772029693\n",
      "Step: 7 Reward: -864.2011214432994\n",
      "Step: 8 Reward: -830.3869934361009\n",
      "Step: 9 Reward: -714.096570737521\n",
      "Step: 10 Reward: -682.1347899743528\n",
      "Step: 11 Reward: -873.6822082963799\n",
      "Step: 12 Reward: -967.185870402649\n",
      "Step: 13 Reward: -787.9147671696422\n",
      "Step: 14 Reward: -282.76344986353905\n",
      "Step: 15 Reward: -319.0088261043764\n",
      "Step: 16 Reward: -233.6336606812511\n",
      "Step: 17 Reward: -253.40530414681146\n",
      "Step: 18 Reward: -305.3761803296549\n",
      "Step: 19 Reward: -125.27802455195847\n",
      "Step: 20 Reward: -280.4372704682304\n",
      "Step: 21 Reward: -253.26914498940658\n",
      "Step: 22 Reward: -115.37445181020708\n",
      "Step: 23 Reward: -188.38291691667033\n",
      "Step: 24 Reward: -182.80078511953016\n",
      "Step: 25 Reward: -175.15636274782145\n",
      "Step: 26 Reward: -146.16987382982907\n",
      "Step: 27 Reward: -202.68822979499183\n",
      "Step: 28 Reward: -76.43388745529576\n",
      "Step: 29 Reward: -124.56838014748608\n",
      "Step: 30 Reward: -127.05083807342542\n",
      "Step: 31 Reward: -153.67243568041863\n",
      "Step: 32 Reward: -91.73719878477733\n",
      "Step: 33 Reward: -32.05445599659531\n",
      "Step: 34 Reward: -23.729207597257755\n",
      "Step: 35 Reward: -13.220245088541995\n",
      "Step: 36 Reward: -9.945321889917135\n",
      "Step: 37 Reward: 52.551369285707246\n",
      "Step: 38 Reward: 74.27297257814863\n",
      "Step: 39 Reward: 195.68613457030898\n",
      "Step: 40 Reward: 122.07962828287982\n",
      "Step: 41 Reward: 75.99327446329376\n",
      "Step: 42 Reward: 205.77265713577896\n",
      "Step: 43 Reward: 278.98417346495546\n",
      "Step: 44 Reward: 285.23914759482767\n",
      "Step: 45 Reward: 182.34352120311632\n",
      "Step: 46 Reward: 190.37994687757498\n",
      "Step: 47 Reward: 296.3496124282637\n",
      "Step: 48 Reward: 133.7389980865817\n",
      "Step: 49 Reward: 299.1268011919693\n",
      "Step: 50 Reward: 319.3619366871547\n",
      "Step: 51 Reward: 246.58241095248306\n",
      "Step: 52 Reward: 223.15822659902275\n",
      "Step: 53 Reward: 368.8632564177838\n",
      "Step: 54 Reward: 220.36798073574306\n",
      "Step: 55 Reward: 400.8888825825304\n",
      "Step: 56 Reward: 343.68253375698936\n",
      "Step: 57 Reward: 330.0039683140359\n"
     ]
    }
   ],
   "source": [
    "# Running the main code\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "\n",
    "hp = Hp()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
